################################ TASK #############################################
# Number of topics: 2 								     #
# 1. Product (3 partitions)                                                       #
# 2. Purchase (6 partitions)							     #
# * Product information retrieved from PostgreSQL database.                       #
# * Purchase information is through datagen source connector with JSON converter. #
###################################################################################

--- Add kafka, connect, and FlinkSQL in docker-compose file. ---
apiVersion: platform.confluent.io/v1beta1
kind: zookeeper
metadata:
  name: zookeeper
  namespace: confluent 
spec:
  replicas: 1 
  image:
    application: confluentinc/cp-zookeeper:7.5.0
    init: confluentinc/confluent-init-container:2.7.0
  dataVolumeCapacity: 3Gi
  logVolumeCapacity: 3Gi
---
apiVersion: platform.confluent.io/v1beta1
kind: Kafka
metadata:
  name: kafka
  namespace: confluent
spec:
  replicas: 1
  image:
    application: confluentinc/cp-server:7.5.0
    init: confluentinc/confluent-init-container:2.7.0
  dataVolumeCapacity: 5Gi
  tls:
    autoGeneratercers: true
  listeners:
    internal:
      authentication:
        type: mtls
      tls:
        enabled: true
    external:
      authentication:
        type: mtls 
      tls: 
        enabled: true
  configOverrides:
    server:
      - "confluent.license.topic.replication.factor=1"
      - "confluent.metrics.reporter.topic.replicas=1" 
      - "confluent.tier.metadata.replication.factor=1"
      - "confluent.metadata.topic.replication.factor=1"
      - "confluent.balancer.topic.replication.factor=1"
      - "confluent.security.event.logger.exporter.kafka.topic.replicas=1"
      - "event.logger.exporter.kafka.topic.replicas=1"
      - "offsets.topic.replication.factor=1"
  metricReporter:
    enabled: true
    bootstrapEndpoint: kafka:9071
    authentication:
      type: mtls
    tls:
      enabled: true
  dependencies:
    zookeeper:
      endpoint: zookeeper.confluent.svc.cluster.local:2181
---
apiVersion: platform.confluent.io/v1beta1
kind: Connect
metadata:
  name: connect
  namespace: confluent
spec:
  replicas: 1
  tls:
    autoGeneratedCerts: true
  image:
    application: confluentinc/cp-server-connect:7.6.0
    init: confluentinc/confluent-init-container:2.8.0
  configOverrides:
    server:
      - config.storage.replication.factor=1
      - offset.storage.replication.factor=1
      - status.storage.replication.factor=1
  build:
    type: onDemand
    onDemand:
      plugins:
        locationType: url
        url:
          - name: kafka-connect-jdbc # the url is used here as we needed to build our own connector lib to include the mysql jdbc jar
            archivePath: https://raw.githubusercontent.com/confluentinc/confluent-kubernetes-examples/master/hybrid/ccloud-JDBC-mysql/confluentinc-kafka-connect-jdbc-10.2.5.zip
            checksum: 9d033fabac89ec0b35a97246f7ca3a36800bcb402ccfbd76adebd4c4c9ca6e7d6a044a9162383a18f99400893581b55a87b9fe89e35276507e9a3fb6cff3fda0
  keyConverterType: io.confluent.connect.avro.AvroConverter
  valueConverterType: io.confluent.connect.avro.AvroConverter
  dependencies:
    kafka:
      bootstrapEndpoint: kafka.confluent.svc.cluster.local:9071
      authentication:
        type: mtls
      tls:
        enabled: true
    schemaRegistry:
      url: https://schemaregistry.confluent.svc.cluster.local:8081
      tls:
        enabled: true

---
version: '3.8'
services:
  sql-client:
    build: sql-client/.
    command: bin/sql-client.sh
    depends_on:
      - jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        rest.address: jobmanager
  jobmanager:
    build: sql-client/.
    ports:
      - "8081:8081"
    command: jobmanager
    volumes:
      - flink_data:/tmp/
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        state.backend: filesystem
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        heartbeat.interval: 1000
        heartbeat.timeout: 5000
        rest.flamegraph.enabled: true
        web.backpressure.refresh-interval: 10000
  taskmanager:
    build: sql-client/.
    depends_on:
      - jobmanager
    command: taskmanager
    volumes:
      - flink_data:/tmp/
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 3
        state.backend: filesystem
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        heartbeat.interval: 1000
        heartbeat.timeout: 5000
volumes:
  flink_data:
--- 
version: '3.7'
services:
  app:
    build: .
    volumes:
      - ./:/app
    environment:
      - POSTGRES_DB=plf_training
      - POSTGRES_USER=platformatory
      - POSTGRES_PASSWORD=plf_password
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
    depends_on:
      - postgres

  postgres:
    image: postgres:14
    ports:
      - 5432:5432
    volumes:
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    environment:
      - POSTGRES_DB=plf_training
      - POSTGRES_USER=platformatory
      - POSTGRES_PASSWORD=plf_password


--- Setup a datagen source connector with the schema defined purchase-schema.json---
{
        "namespace": "datagen.example",
        "name": "purchase",
        "type": "record",
        "fields": [
                {"name": "id", "type": {
                    "type": "long",
                    "arg.properties": {
                        "iteration": {
                            "start": 0
                        }
                    }
                }},
                {"name": "product_id", "type": {
                    "type": "string",
                       "arg.properties": {
                        "min":1,
                        "max":100
                     }
                }},
                {"name":"quantity",
                  "type":{
                     "type":"long",
                     "arg.properties":{
                        "range":{
                           "min":1,
                           "max":10
                        }
                     }
                 }},
                 {"name": "customer_id", "type": {
                  "type": "string",
                     "arg.properties": {
                      "min":1,
                      "max":100
                   }
              }},
              {"name": "discount", "type": {
                "type": "double",
                 "arg.properties": {
                    "range":{
                       "min":0,
                       "max":50
                    }
                 }
              }}
        ]
}

---- Write the connector.json file --- 
{
  "name": "datagen",
  "config": {
    "kafka.topic": "purchase",
    "tasks.max": "10",
    "connector.class": "io.confluent.kafka.connect.datagen.DatagenConnector",
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "key.converter.schema.registry.url": "http://schema-registry:8085",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter.schema.registry.url": "http://schema-registry:8085",
    "schema.filename": "/home/platformatory/purchase-schema.avsc",
    "schema.keyfield": "id",
    "errors.tolerance": "all"
  }
}


--- Run the docker compose file --- 
docker compose up -d 

--- check if the product table is there or not --- 
docker compose exec -it postgres psql "postgres://platformatory:plf_password@postgres:5432/plf_training"

#- NOTE: if it's not there , run the app service and check again. -# 
docker compose run -d app 

--- Run the kafka container and create the topic --- 
docker compose exec kafka-1 /bin/bash 

kafka-topics --bootstrap-server kafka-1:9092 --create --topic purchase --partitions 6 --command-config /home/platformatory/client.properties 

--- Run the connect container and create the connector --- 
docker compose exec connect /bin/bash 

cd /home/platformatory
curl -X POST -H "Content-Type: application/json" --data @connector.json http://localhost:8083/connectors

## Check the connect status 
kafka-console-consumer --bootstrap-server kafka-1:9092 --topic docker-connect-status --consumer.config /home/platformatory/client.properties --from-beginning 

## Check if the data is there in the topic 
kafka-avro-console-consumer --bootstrap-server kafka-1:9092 --topic top --consumer.config /home/platformatory/client.properties --property schema.registry.url=http://schema-registry:8085 --from-beginning

curl -X DELETE http://localhost:8083/connectors

--- Run the Flink SQL --- 
docker compose run sql-client 

--- Create the source connector in Apache Flink ---- 
CREATE TABLE product (
`id` INT,
`created` TIMESTAMP(3), 
`modified` TIMESTAMP(3),
`name` VARCHAR,
`category` VARCHAR, 
`price` DOUBLE PRECISION, 
WATERMARK FOR `created` AS `created`
) WITH (
'connector' = 'jdbc',
'url' = 'jdbc:postgresql://postgres:5432/plf_training',
'table-name' = 'product',
'username' = 'platformatory',
'password' = 'plf_password'
);

--- Creat the table for purchase --- 

CREATE TABLE purchase (
        `id` BIGINT,
        `product_id` BIGINT,
        `quantity` BIGINT,
        `customer_id` STRING,
        `discount` DOUBLE, 
        `created_at` TIMESTAMP(3) METADATA FROM 'timestamp', 
        WATERMARK FOR `created_at` AS `created_at`
) WITH (
    'connector' = 'kafka',
    'topic' = 'purchase',
    'scan.startup.mode' = 'earliest-offset',
    'properties.bootstrap.servers' = 'kafka-1:9092',
    'properties.security.protocol' = 'SASL_PLAINTEXT',
    'properties.sasl.mechanism' = 'PLAIN',
    'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="admin-secret";',
    'format' = 'avro-confluent',
    'avro-confluent.schema-registry.url' = 'http://schema-registry:8085',
    'avro-confluent.schema-registry.subject' = 'purchase-value'
);

--- Create the result table --- 

CREATE TABLE sales ( 
`window_start` TIMESTAMP(3), 
`window_end` TIMESTAMP(3),
`product_id` BIGINT, 
`category` STRING, 
`price` DOUBLE,
`total` DOUBLE,
PRIMARY KEY(product_id) NOT ENFORCED
) WITH ( 
'connector' = 'upsert-kafka', 
'topic' = 'sales', 
'properties.bootstrap.servers' = 'kafka-1:9092', 
'properties.group.id' = 'consumer-sales-group',
'properties.security.protocol' = 'SASL_PLAINTEXT',
'properties.sasl.mechanism' = 'PLAIN',
'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="admin-secret";',
'key.format' = 'avro-confluent',
'properties.allow.auto.create.topics' = 'false',
'key.avro-confluent.schema-registry.url' = 'http://schema-registry:8085',
'key.avro-confluent.schema-registry.subject' = 'top-sales-value',
'value.format' = 'avro-confluent',
'value.avro-confluent.schema-registry.url' = 'http://schema-registry:8085',
'value.avro-confluent.schema-registry.subject' = 'top-sales-value',
'properties.auto.offset.reset' = 'earliest'
);
    


---- Now write a query to return top product -- 

#- Change the streaming mode -# 
set 'execution.runtime-mode' = 'streaming';
set 'sql-client.execution.result-mode' = 'changelog';


--- Insert the data into result table --------
INSERT INTO sales(window_start, window_end, product_id, category, price, total) 
SELECT window_start, window_end, pr.product_id, p.category, p.price, 
SUM((p.price * pr.quantity - (1 - COALESCE(pr.discount,0)/100))) AS total
FROM TABLE(HOP(TABLE purchase, DESCRIPTOR(ts), INTERVAL '5' MINUTE, INTERVAL '1' HOUR)) AS pw 
JOIN purchase pr ON pr.product_id = pw.id 
JOIN product p ON p.id = pw.id
GROUP BY window_start, window_end, pr.product_id, p.category, p.price;

---- Run the query to find the top performing product ---- 
SELECT * FROM ( 
SELECT * , ROW_NUMBER() OVER (ORDER BY window_start ASC) AS rownum 
FROM( 
select * from sales group by window_start, window_end, product_id, category, price, total ORDER BY total DESC) 
) where rownum = 1;
